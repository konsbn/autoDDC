import json
import os
import mechanicalsoup
from itertools import chain
from collections import OrderedDict
from isbntools.app import meta


browser = mechanicalsoup.StatefulBrowser()


class scrapertools:
    '''
    Provides a suite of tools for opening web pages
    with scraping and cleanup tools.
    '''

    def __init__(self):
        '''
        Initializes the scrapertools class with the browser instance
        '''
        self.browser = mechanicalsoup.StatefulBrowser()

    def page_getter(self, url):
        '''
        Returns the current page of the url to be opened
        '''
        self.base_url = '/'.join(url.split('/')[:-1])  # Gets the base url
        self.browser.open(url)
        self.page = self.browser.get_current_page()

    def _scraper(self, html):
        '''
        A cleanup utility for detagging html and checks for links
        '''
        if html.find('a', href=True) is not None:
            return html.contents[0].string, html.find('a', href=True)['href']
        return html.string

    def _table_cleaner(self, od):
        '''
        Performs the cleanup for the results table so it gives both the title
        and the link separately
        '''
        title, link = od['Title & Author']
        od.update(OrderedDict([('Title & Author', title)]))
        od.update(OrderedDict(
            [('Link', self.base_url + '/' + '/'.join(link.split('/')[2:]))]))
        return od

    def table_getter(self, ids):
        '''
        Gets the tables based on id from the current page.
        '''

        table = self.page.find('table', {'id': ids})  # Finds table by id
        head = table.find('thead')  # Finds the table head
        body = table.find('tbody')  # Finds the body of the table
        # Extracts the headers from table head
        headers = [tag.string for tag in head.find_all('th')]
        # Finds all the titles in the body
        body = [entry.find_all('td')for entry in body.find_all('tr')]
        # Removes Empty Elements
        if [] in body:
            body.remove([])
        body = [[self._scraper(j) for j in i] for i in body]
        body = [list(zip(headers, j)) for j in body]
        self.table = OrderedDict({i: OrderedDict(
            {j[0]: j[1] for j in body[i]})for i in range(len(body))})
        if ids == 'results-table':
            for i in self.table.keys():
                self._table_cleaner(self.table[i])
        return self.table



def get_title_author(page):
    '''
    Gets the title and author from the page from the item summary
    '''
    item_summary = page.find('div', {'id': 'display-Summary'})
    headers = [i.text for i in item_summary.find_all('dt')]
    entries = [i.text for i in item_summary.find_all('dd')]
    table = dict(zip(headers, entries))
    # author = ','.join(table['Author:'].split(',')[:-1])
    author = table['Author:']
    author = ' '.join(author.split(';')[0].split(',')[::-1][1:])
    title = table['Title:']
    return title, author


def _categorylist(ddc):
    '''Just Gets The Broad classes upto the tenth place given broad DDC Class
       eg:

       DDC 500 will fetch everything till 599'''

    # Goes to the Summary Page of OCLC
    browser.open('https://www.oclc.org/en/dewey/resources/summaries.html')
    page = browser.get_current_page()
    cat = page.find('a', {'name': str(ddc)}).find_next(
        'tr').contents  # Finds the Table of DDC and writes it to cat
    while '\n' in cat:  # Basic Cleanup
        cat.remove('\n')
    cat = [i.string for i in (
        cat[0].contents + cat[1].contents) if i.string is not None]
    return(cat)


def GetAllCategory():
    '''
    Gets all the DDC Categories from 000 to 999
    Output : OrderedDict[DDC] = 'Genre'
    '''
    DDC_Categories = ['000', '100', '200', '300',
                      '400', '500', '600', '700', '800', '900']
    # Fetch DDC Genres For all the Broad Categories and Chain
    # together in one list. String cleanup is performed to strip
    # whitespaces. Should have done it more explicitly
    CatList = [i.strip() for i in list(chain.from_iterable(
        map(_categorylist, DDC_Categories))) if i != ' ']
    # CatList -> Complete_DDC_Categories[DDC] = Genre
    Complete_DDC_Categories = OrderedDict()
    for i in CatList:
        ddc_class, genre = i.split(' ')[0], ' '.join(i.split(' ')[1:])
        Complete_DDC_Categories.update(OrderedDict([(ddc_class, genre)]))
    return(Complete_DDC_Categories)


def store_categories(filename='ddc_summary.json'):
    with open(filename, 'w') as outfile:
        json.dump(GetAllCategory(), outfile)


def read_categories(filename='ddc_summary.json'):
    return json.load(open('ddc_summary.json'), object_pairs_hook=OrderedDict)


def check_if_exists(path='./'):
    return os.path.isfile(path+'ddc_summary.json')


def get_summaries():
    if check_if_exists():
        return read_categories()
    else:
        store_categories()
        return read_categories()


class ISBNError(Exception):
    '''
    Raises an exception when ISBN is invalid
    '''

    def __init__(self, expression, message):
        self.expression = expression
        self.message = message


def isddc(ddc):
    try:
        float(ddc)
        return True
    except ValueError:
        return False


def isValid(isbn):
    '''
    Returns : True is Valid ISBN else False
    Eg:
    >>> isValid(9780231175968)
    True
    >>> isValid(123)
    Traceback (most recent call last):
        ...
    ISBNError: ('123', 'ISBN is Invalid')
    '''

    isbn = list(str(isbn))  # We need to iterate over this!
    if len(isbn) == 13:
        isbn = [int(j)*1 if (int(i)+1) % 2 != 0 else int(j)
                * 3 for i, j in enumerate(isbn)]
        if sum(isbn) % 10 == 0:
            return
        else:
            raise ISBNError(''.join(isbn), 'ISBN is Invalid')
    else:
        raise ISBNError(''.join(isbn), 'ISBN is Invalid')


class AutoDDC:
    def __init__(self):
        '''
        Initializes the Browser Class
        '''
        self.browser = mechanicalsoup.StatefulBrowser()
        self.getter = scrapertools()
        self.summaries = get_summaries()

    def find_in_url(self, url):
        '''
        Finds the DDC in the current page or returns the list of links to follow
        '''
        self.getter.page_getter(url)
        try:
            self.cat = self.getter.table_getter('classSummaryData')
            # if self.cat is not None:
            return self.cat
        except AttributeError:
            self.links = self.getter.table_getter('results-table')
            return self.links

    def ddc_urlwrapper(self, isbn):
        '''
        Gets the Dewey Decimal Classification given isbn
        Input
        >>> isbn
        Output
        DDC
        '''
        url = "http://classify.oclc.org"
        url1 = url + "/classify2/ClassifyDemo?search-standnum-txt=" + \
            str(isbn) + "&startRec=0"
        return url1

    def pretty_ddc(self):
        try:
            for i in self.cat.values():
                if i['DDC:'] == 'Most Frequent':
                    if isddc(i['Class Number']):
                        return i['Class Number']
        except AttributeError:

            self.find_in_url(self.links[0]['Link'])
            for i in self.cat.values():
                if i['DDC:'] == 'Most Frequent':
                    if isddc(i['Class Number']):
                        return i['Class Number']



def wrapper(isbn, genre=True):
    try:
        ddc_scraper = AutoDDC()
        ddc_scraper.find_in_url(ddc_scraper.ddc_urlwrapper(isbn))
        ddc = ddc_scraper.pretty_ddc()
        current_page = ddc_scraper.getter.page

        if genre:
            genre = ddc_scraper.summaries[str(int(float(ddc)))]
            try:
                metadata = meta(str(isbn))
                title, author = metadata['Title'], metadata['Authors'][0]
            except:
                metadata = get_title_author(current_page)
                title, author = metadata

            return OrderedDict([('DDC', ddc), ('Title', title), ('Author', author),  ('Genre', genre)])
        return ddc
    except ConnectionError:
        return('Internet Not Connected')
    except AttributeError:
        return(f'No Entry for {isbn}')
